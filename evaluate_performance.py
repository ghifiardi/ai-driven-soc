import argparse
import logging
from google.cloud import firestore
import pandas as pd
from datetime import datetime, timezone

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def save_performance_report(db: firestore.Client, metrics: dict):
    """Saves the performance report to the 'model_performance' collection."""
    try:
        # Use a timestamp for the document ID to keep records chronological
        report_id = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M-%S")
        
        # Add a server timestamp for accurate record-keeping
        report_with_timestamp = metrics.copy()
        report_with_timestamp['evaluation_timestamp'] = firestore.SERVER_TIMESTAMP
        
        db.collection("model_performance").document(report_id).set(report_with_timestamp)
        logger.info(f"Successfully saved performance report to Firestore with ID: {report_id}")
    except Exception as e:
        logger.error(f"Failed to save performance report: {e}")

def fetch_collection_data(db: firestore.Client, collection_name: str) -> list:
    """Fetches all documents from a specified Firestore collection."""
    logger.info(f"Fetching all documents from '{collection_name}' collection...")
    docs = db.collection(collection_name).stream()
    data = [doc.to_dict() for doc in docs]
    logger.info(f"Successfully fetched {len(data)} documents.")
    return data

def evaluate_performance(project_id: str):
    """
    Compares AI-generated alerts with human-labeled ground truth to calculate performance metrics.
    """
    try:
        db = firestore.Client(project=project_id)
    except Exception as e:
        logger.critical(f"Failed to initialize Firestore client: {e}")
        return

    # 1. Fetch data from both collections
    # For this evaluation, we only need the ground truth data, as it contains the AI's original alert
    # and the human's final classification.
    ground_truth_data = fetch_collection_data(db, "ground_truth_alerts")

    if not ground_truth_data:
        logger.warning("The 'ground_truth_alerts' collection is empty. No data to evaluate.")
        return

    # 2. Convert to a Pandas DataFrame for easier analysis
    df = pd.DataFrame(ground_truth_data)

    # For this analysis, we consider both 'false_positive' and 'benign' as not a real threat.
    df['is_true_positive'] = df['classification'] == 'true_positive'

    # 3. Calculate metrics
    total_evaluated = len(df)
    true_positives = df['is_true_positive'].sum()
    # Everything that is not a true positive is considered a false positive in this context
    false_positives = total_evaluated - true_positives

    # --- Calculate Precision ---
    # Precision = TP / (TP + FP)
    # This metric answers: "Of all the alerts that the AI flagged, what percentage were correct?"
    if (true_positives + false_positives) > 0:
        precision = (true_positives / (true_positives + false_positives)) * 100
    else:
        precision = 0

    # Note on Recall: To calculate recall (TP / (TP + FN)), we would need to know about
    # False Negatives (FN) - alerts that the AI *missed* but were actual threats.
    # This requires a separate process to identify and is not included in this script.

    # 4. Prepare and display the performance report
    report_metrics = {
        "total_evaluated": int(total_evaluated),
        "true_positives": int(true_positives),
        "false_positives": int(false_positives),
        "precision": float(precision)
    }

    print("\n--- ADA Performance Report ---")
    print("=======================================")
    print(f"Total Labeled Alerts Evaluated: {report_metrics['total_evaluated']}")
    print("---------------------------------------")
    print(f"  - True Positives (TP):  {report_metrics['true_positives']}")
    print(f"  - False Positives (FP): {report_metrics['false_positives']}")
    print("=======================================")
    print(f"Precision: {report_metrics['precision']:.2f}%")
    print("\nExplanation:")
    print(f"> Of the {report_metrics['total_evaluated']} alerts generated by ADA and reviewed by an analyst, {report_metrics['precision']:.2f}% were correctly identified as real threats.")
    print("---------------------------------------\n")
    
    # 5. Save the report to Firestore
    save_performance_report(db, report_metrics)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the performance of the ADA system against ground truth data.")
    parser.add_argument("--project_id", type=str, default="chronicle-dev-2be9", help="The Google Cloud project ID.")
    args = parser.parse_args()

    evaluate_performance(project_id=args.project_id)
